{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This notebook implements the Madras et al. 2018 baseline on CIFAR-10. Running this notebook will produce results for a single expert k."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "P_hZCi7E4J1o"
   },
   "source": [
    "# Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "pk72jE5icG-B",
    "outputId": "65c861d9-e777-4c84-e275-314655826c05"
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import torch\n",
    "import numpy as np\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import argparse\n",
    "import os\n",
    "import shutil\n",
    "import time\n",
    "import torch.nn.parallel\n",
    "import torch.backends.cudnn as cudnn\n",
    "import torch.optim\n",
    "import torch.utils.data\n",
    "import torchvision.transforms as transforms\n",
    "import torchvision.datasets as datasets\n",
    "from torch.autograd import Variable\n",
    "\n",
    "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(device)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "b514O4uAcO4K"
   },
   "outputs": [],
   "source": [
    "class BasicBlock(nn.Module):\n",
    "    def __init__(self, in_planes, out_planes, stride, dropRate=0.0):\n",
    "        super(BasicBlock, self).__init__()\n",
    "        self.bn1 = nn.BatchNorm2d(in_planes)\n",
    "        self.relu1 = nn.ReLU(inplace=True)\n",
    "        self.conv1 = nn.Conv2d(in_planes, out_planes, kernel_size=3, stride=stride,\n",
    "                               padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(out_planes)\n",
    "        self.relu2 = nn.ReLU(inplace=True)\n",
    "        self.conv2 = nn.Conv2d(out_planes, out_planes, kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        self.droprate = dropRate\n",
    "        self.equalInOut = (in_planes == out_planes)\n",
    "        self.convShortcut = (not self.equalInOut) and nn.Conv2d(in_planes, out_planes, kernel_size=1, stride=stride,\n",
    "                                                                padding=0, bias=False) or None\n",
    "    def forward(self, x):\n",
    "        if not self.equalInOut:\n",
    "            x = self.relu1(self.bn1(x))\n",
    "        else:\n",
    "            out = self.relu1(self.bn1(x))\n",
    "        out = self.relu2(self.bn2(self.conv1(out if self.equalInOut else x)))\n",
    "        if self.droprate > 0:\n",
    "            out = F.dropout(out, p=self.droprate, training=self.training)\n",
    "        out = self.conv2(out)\n",
    "        return torch.add(x if self.equalInOut else self.convShortcut(x), out)\n",
    "\n",
    "\n",
    "class NetworkBlock(nn.Module):\n",
    "    def __init__(self, nb_layers, in_planes, out_planes, block, stride, dropRate=0.0):\n",
    "        super(NetworkBlock, self).__init__()\n",
    "        self.layer = self._make_layer(block, in_planes, out_planes, nb_layers, stride, dropRate)\n",
    "    def _make_layer(self, block, in_planes, out_planes, nb_layers, stride, dropRate):\n",
    "        layers = []\n",
    "        for i in range(int(nb_layers)):\n",
    "            layers.append(block(i == 0 and in_planes or out_planes, out_planes, i == 0 and stride or 1, dropRate))\n",
    "        return nn.Sequential(*layers)\n",
    "    def forward(self, x):\n",
    "        return self.layer(x)\n",
    "\n",
    "\n",
    "class WideResNet(nn.Module):\n",
    "    def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n",
    "        super(WideResNet, self).__init__()\n",
    "        nChannels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]\n",
    "        assert ((depth - 4) % 6 == 0)\n",
    "        n = (depth - 4) / 6\n",
    "        block = BasicBlock\n",
    "        # 1st conv before any network block\n",
    "        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        # 1st block\n",
    "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n",
    "        # 2nd block\n",
    "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
    "        # 3rd block\n",
    "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
    "        # global average pooling and classifier\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels[3])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc = nn.Linear(nChannels[3], num_classes)\n",
    "        self.nChannels = nChannels[3]\n",
    "        self.softmax = nn.Softmax()\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.relu(self.bn1(out))\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(-1, self.nChannels)\n",
    "        out = self.fc(out)\n",
    "        out = self.softmax(out)\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We modify the WideResNet architecture to implement the baseline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "om0Cr0lGAM-D"
   },
   "outputs": [],
   "source": [
    "class WideResNet_madras(nn.Module):\n",
    "    def __init__(self, depth, num_classes, widen_factor=1, dropRate=0.0):\n",
    "        super(WideResNet_madras, self).__init__()\n",
    "        nChannels = [16, 16 * widen_factor, 32 * widen_factor, 64 * widen_factor]\n",
    "        assert ((depth - 4) % 6 == 0)\n",
    "        n = (depth - 4) / 6\n",
    "        block = BasicBlock\n",
    "        # 1st conv before any network block\n",
    "        self.conv1 = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n",
    "                               padding=1, bias=False)\n",
    "        # 1st block\n",
    "        self.block1 = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n",
    "        # 2nd block\n",
    "        self.block2 = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
    "        # 3rd block\n",
    "        self.block3 = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
    "        # global average pooling and classifier\n",
    "        self.bn1 = nn.BatchNorm2d(nChannels[3])\n",
    "        self.relu = nn.ReLU(inplace=True)\n",
    "        self.fc = nn.Linear(nChannels[3], num_classes)\n",
    "        self.nChannels = nChannels[3]\n",
    "        self.softmax = nn.Softmax()\n",
    "\n",
    "        # 1st conv before any network block\n",
    "        self.conv1_rej = nn.Conv2d(3, nChannels[0], kernel_size=3, stride=1,\n",
    "                                   padding=1, bias=False)\n",
    "        # 1st block\n",
    "        self.block1_rej = NetworkBlock(n, nChannels[0], nChannels[1], block, 1, dropRate)\n",
    "        # 2nd block\n",
    "        self.block2_rej = NetworkBlock(n, nChannels[1], nChannels[2], block, 2, dropRate)\n",
    "        # 3rd block\n",
    "        self.block3_rej = NetworkBlock(n, nChannels[2], nChannels[3], block, 2, dropRate)\n",
    "        # global average pooling and classifier\n",
    "        self.bn1_rej = nn.BatchNorm2d(nChannels[3])\n",
    "        self.relu_rej = nn.ReLU(inplace=True)\n",
    "        self.fc_rej = nn.Linear(nChannels[3], 2)\n",
    "        self.softmax_rej = nn.Softmax()\n",
    "        for m in self.modules():\n",
    "            if isinstance(m, nn.Conv2d):\n",
    "                nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')\n",
    "            elif isinstance(m, nn.BatchNorm2d):\n",
    "                m.weight.data.fill_(1)\n",
    "                m.bias.data.zero_()\n",
    "            elif isinstance(m, nn.Linear):\n",
    "                m.bias.data.zero_()\n",
    "\n",
    "    def forward(self, x):\n",
    "        out = self.conv1(x)\n",
    "        out = self.block1(out)\n",
    "        out = self.block2(out)\n",
    "        out = self.block3(out)\n",
    "        out = self.relu(self.bn1(out))\n",
    "        out = F.avg_pool2d(out, 8)\n",
    "        out = out.view(-1, self.nChannels)\n",
    "        out = self.fc(out)\n",
    "        out = self.softmax(out)\n",
    "\n",
    "        rej = self.conv1_rej(x)\n",
    "        rej = self.block1_rej(rej)\n",
    "        rej = self.block2_rej(rej)\n",
    "        rej = self.block3_rej(rej)\n",
    "        rej = self.relu_rej(self.bn1_rej(rej))\n",
    "        rej = F.avg_pool2d(rej, 8)\n",
    "        rej = rej.view(-1, self.nChannels)\n",
    "        rej = self.fc_rej(rej)\n",
    "        rej = self.softmax_rej(rej)\n",
    "\n",
    "        return [out, rej]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ZrijgT1NcRC3"
   },
   "outputs": [],
   "source": [
    "class AverageMeter(object):\n",
    "    \"\"\"Computes and stores the average and current value\"\"\"\n",
    "    def __init__(self):\n",
    "        self.reset()\n",
    "\n",
    "    def reset(self):\n",
    "        self.val = 0\n",
    "        self.avg = 0\n",
    "        self.sum = 0\n",
    "        self.count = 0\n",
    "\n",
    "    def update(self, val, n=1):\n",
    "        self.val = val\n",
    "        self.sum += val * n\n",
    "        self.count += n\n",
    "        self.avg = self.sum / self.count\n",
    "\n",
    "def accuracy(output, target, topk=(1,)):\n",
    "    \"\"\"Computes the precision@k for the specified values of k\"\"\"\n",
    "    maxk = max(topk)\n",
    "    batch_size = target.size(0)\n",
    "\n",
    "    _, pred = output.topk(maxk, 1, True, True)\n",
    "    pred = pred.t()\n",
    "    correct = pred.eq(target.view(1, -1).expand_as(pred))\n",
    "\n",
    "    res = []\n",
    "    for k in topk:\n",
    "        correct_k = correct[:k].view(-1).float().sum(0)\n",
    "        res.append(correct_k.mul_(100.0 / batch_size))\n",
    "    return res\n",
    "\n",
    "import random\n",
    "\n",
    "def metrics_madras(net, expert_fn, n_classes, loader):\n",
    "    correct = 0\n",
    "    correct_sys = 0\n",
    "    exp = 0\n",
    "    exp_total = 0\n",
    "    total = 0\n",
    "    real_total = 0\n",
    "    alone_correct = 0\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            images, labels = data\n",
    "            images, labels = images.to(device), labels.to(device)\n",
    "            outputs, rej = net(images)\n",
    "            _, predicted = torch.max(outputs.data, 1)\n",
    "            batch_size = outputs.size()[0]  # batch_size\n",
    "            exp_prediction = expert_fn(images, labels)\n",
    "            for i in range(0,batch_size):\n",
    "                r = (rej[i][0].item() >= 0.5)\n",
    "                if r == 0:\n",
    "                    total += 1\n",
    "                    correct += (predicted[i] == labels[i]).item()\n",
    "                    correct_sys += (predicted[i] == labels[i]).item()\n",
    "                if r == 1:\n",
    "                    exp += (exp_prediction[i] == labels[i].item())\n",
    "                    correct_sys += (exp_prediction[i] == labels[i].item())\n",
    "                    exp_total += 1\n",
    "                real_total += 1\n",
    "                alone_correct += (predicted[i] == labels[i]).item()\n",
    "    cov = str(total) + str(\" out of\") + str(real_total)\n",
    "    to_print = {\"coverage\": cov, \"system accuracy\": 100 * correct_sys / real_total,\n",
    "                \"expert accuracy\": 100 * exp / (exp_total + 0.0002),\n",
    "                \"classifier accuracy\": 100 * correct / (total + 0.0001),\n",
    "                \"alone classifier\": 100 * alone_correct / real_total}\n",
    "    print(to_print)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xFT0AoWD4nSa"
   },
   "source": [
    "# Expert and data loaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 54
    },
    "colab_type": "code",
    "id": "rlKWAHpFcWDA",
    "outputId": "b599df5e-c52b-4e35-99aa-47f9069fe28b"
   },
   "outputs": [],
   "source": [
    "normalize = transforms.Normalize(mean=[x/255.0 for x in [125.3, 123.0, 113.9]],\n",
    "                                        std=[x/255.0 for x in [63.0, 62.1, 66.7]])\n",
    "\n",
    "transform_train = transforms.Compose([\n",
    "        transforms.ToTensor(),\n",
    "        normalize,\n",
    "        ])\n",
    "transform_test = transforms.Compose([\n",
    "    transforms.ToTensor(),\n",
    "    normalize\n",
    "    ])\n",
    "\n",
    "n_dataset = 10\n",
    "dataset = 'cifar10' \n",
    "kwargs = {'num_workers': 1, 'pin_memory': True}\n",
    "\n",
    "\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader( \n",
    "    datasets.__dict__[dataset.upper()]('../data', train=True, download=True,\n",
    "                                    transform=transform_train),\n",
    "                    batch_size=128, shuffle=True, **kwargs)\n",
    "\n",
    "train_dataset_all = datasets.__dict__[dataset.upper()]('../data', train=True, download=True,\n",
    "                                    transform=transform_train)\n",
    "\n",
    "train_size = int(0.90 * len(train_dataset_all))\n",
    "train_size_val = len(train_dataset_all) - train_size\n",
    "\n",
    "train_dataset, train_dataset_val = torch.utils.data.random_split(train_dataset_all, [train_size, train_size_val])\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=128, shuffle=True, **kwargs)\n",
    "\n",
    "train_loader_val = torch.utils.data.DataLoader(\n",
    "    train_dataset_val,\n",
    "    batch_size=128, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "val_loader = torch.utils.data.DataLoader(\n",
    "    datasets.__dict__[dataset.upper()]('../data', train=False, transform=transform_test),\n",
    "    batch_size=128, shuffle=True, **kwargs)\n",
    "\n",
    "\n",
    "class synth_expert:\n",
    "    def __init__(self, k, n_classes):\n",
    "        self.k = k\n",
    "        self.n_classes = n_classes\n",
    "\n",
    "    def predict(self, input, labels):\n",
    "        batch_size = labels.size()[0]  # batch_size\n",
    "        outs = [0] * batch_size\n",
    "        for i in range(0,batch_size):\n",
    "            if labels[i].item() <= self.k:\n",
    "                #coin = np.random.binomial(1,0.94,1)[0]\n",
    "                #if coin:\n",
    "                outs[i] = labels[i].item()\n",
    "                #else:\n",
    "                 #   prediction_rand = random.randint(0, self.n_classes - 1)\n",
    "                 #   outs[i] = prediction_rand\n",
    "            else:\n",
    "                prediction_rand = random.randint(0, self.n_classes - 1)\n",
    "                outs[i] = prediction_rand\n",
    "        return outs\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 5 # expert parameter\n",
    "expert = synth_expert(k, n_dataset)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "bEZo4RC4_3d4"
   },
   "source": [
    "# Baseline: Madras et al. 2018"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "u5MT-Rxr_5Cw"
   },
   "outputs": [],
   "source": [
    "def madras_loss(outputs, rej, labels, expert, eps = 10e-12):\n",
    "    # MixOfExperts loss of Madras et al. 2018\n",
    "    batch_size = outputs.size()[0]\n",
    "    output_no_grad = outputs.detach()\n",
    "    net_loss_no_grad = -torch.log2(output_no_grad[range(batch_size), labels]+eps)\n",
    "    net_loss = -torch.log2(outputs[range(batch_size), labels]+eps)\n",
    "    exp_loss = -torch.log2(expert[range(batch_size), labels]+eps) \n",
    "    system_loss =  (rej[range(batch_size),0])  *  net_loss_no_grad + rej[range(batch_size),1]  * exp_loss\n",
    "    system_loss += net_loss\n",
    "    return torch.sum(system_loss)/batch_size\n",
    "\n",
    "\n",
    "\n",
    "def cross_entropy_loss(outputs, labels):\n",
    "    batch_size = outputs.size()[0]\n",
    "    net_loss = -torch.log2(outputs[range(batch_size), labels])\n",
    "    return torch.sum(net_loss)/batch_size\n",
    "\n",
    "\n",
    "def train_madras(train_loader, model, optimizer, scheduler, epoch, expert_fn, n_classes):\n",
    "    \"\"\"Train for one epoch on the training set\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to train mode\n",
    "    model.train()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(train_loader):\n",
    "        target = target.to(device)\n",
    "        input = input.to(device)\n",
    "\n",
    "        # compute output\n",
    "        output, rej = model(input)\n",
    "        output.detach()\n",
    "        # expert  predictions\n",
    "        expert_pred = expert_fn(input, target)\n",
    "        expert_pred_n = [[0.005555]*10] * output.size()[0]\n",
    "        for k in range(len(expert_pred)):\n",
    "            if target[k].item() ==expert_pred[k]:\n",
    "                expert_pred_n[k][target[k].item()] = 1 - 10e-12\n",
    "            else:\n",
    "                expert_pred_n[k] = [0.1] * 10\n",
    "        expert_pred_n = torch.tensor(expert_pred_n)\n",
    "        expert_pred_n = expert_pred_n.to(device)\n",
    "\n",
    "        loss = madras_loss(output, rej, target, expert_pred_n)\n",
    "\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target, topk=(1,))[0]\n",
    "        losses.update(loss.data.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # compute gradient and do SGD step\n",
    "        #loss.backward()\n",
    "        optimizer.step()\n",
    "        scheduler.step()\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Epoch: [{0}][{1}/{2}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      epoch, i, len(train_loader), batch_time=batch_time,\n",
    "                      loss=losses, top1=top1))\n",
    "\n",
    "\n",
    "def validate_madras(val_loader, model, epoch, expert_fn, n_classes):\n",
    "    \"\"\"Perform validation on the validation set\"\"\"\n",
    "    batch_time = AverageMeter()\n",
    "    losses = AverageMeter()\n",
    "    top1 = AverageMeter()\n",
    "\n",
    "    # switch to evaluate mode\n",
    "    model.eval()\n",
    "\n",
    "    end = time.time()\n",
    "    for i, (input, target) in enumerate(val_loader):\n",
    "        target = target.to(device)\n",
    "        input = input.to(device)\n",
    "\n",
    "        # compute output\n",
    "        with torch.no_grad():\n",
    "            output, rej = model(input)\n",
    "        # expert prediction\n",
    "                # compute output\n",
    "\n",
    "        # expert  predictions\n",
    "        expert_pred = model_expert(input)\n",
    "        expert_pred = expert_pred.to(device)\n",
    "        loss = madras_loss(output, rej, target, expert_pred)\n",
    "\n",
    "        # measure accuracy and record loss\n",
    "        prec1 = accuracy(output.data, target, topk=(1,))[0]\n",
    "        losses.update(loss.data.item(), input.size(0))\n",
    "        top1.update(prec1.item(), input.size(0))\n",
    "\n",
    "        # measure elapsed time\n",
    "        batch_time.update(time.time() - end)\n",
    "        end = time.time()\n",
    "\n",
    "        if i % 10 == 0:\n",
    "            print('Test: [{0}/{1}]\\t'\n",
    "                  'Time {batch_time.val:.3f} ({batch_time.avg:.3f})\\t'\n",
    "                  'Loss {loss.val:.4f} ({loss.avg:.4f})\\t'\n",
    "                  'Prec@1 {top1.val:.3f} ({top1.avg:.3f})'.format(\n",
    "                      i, len(val_loader), batch_time=batch_time, loss=losses,\n",
    "                      top1=top1))\n",
    "\n",
    "    print(' * Prec@1 {top1.avg:.3f}'.format(top1=top1))\n",
    "\n",
    "    return top1.avg\n",
    "best_prec1 = 0\n",
    "def run_madras(model, data_aug, n_dataset, expert_fn, epochs):\n",
    "    global best_prec1\n",
    "\n",
    "    # get the number of model parameters\n",
    "    print('Number of model parameters: {}'.format(\n",
    "        sum([p.data.nelement() for p in model.parameters()])))\n",
    "\n",
    "    # for training on multiple GPUs.\n",
    "    # Use CUDA_VISIBLE_DEVICES=0,1 to specify which GPUs to use\n",
    "    # model = torch.nn.DataParallel(model).cuda()\n",
    "    model = model.to(device)\n",
    "\n",
    "    # optionally resume from a checkpoint\n",
    "    \n",
    "\n",
    "    cudnn.benchmark = True\n",
    "\n",
    "    # define loss function (criterion) and optimizer\n",
    "    optimizer = torch.optim.SGD(model.parameters(), 0.1,\n",
    "                                momentum=0.9, nesterov = True,\n",
    "                                weight_decay=5e-4)\n",
    "\n",
    "    # cosine learning rate\n",
    "    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, len(train_loader)*200)\n",
    "\n",
    "\n",
    "    prec1 = 0\n",
    "    for epoch in range(0, epochs):\n",
    "        # train for one epoch\n",
    "        train_madras(train_loader, model, optimizer, scheduler, epoch, expert_fn, n_dataset)\n",
    "\n",
    "\n",
    "    print('Best accuracy: ', best_prec1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dF8ErkKUCMd1"
   },
   "outputs": [],
   "source": [
    "model = WideResNet_madras(10, n_dataset, 4, dropRate=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "colab_type": "code",
    "id": "IDD6aDI_COIH",
    "outputId": "cc6aab20-3663-463f-901e-ec3c680ea2c9"
   },
   "outputs": [],
   "source": [
    "run_madras(model, True, n_dataset, expert.predict, 220)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Evaluate\n",
    "- coverage: percentage of examples where classifier predicts\n",
    "- classifier accuracy: accuracy of classifier on non-deferred examples\n",
    "- expert accuracy: accuracy of expert on deferred examples\n",
    "- classifier alone accuracy: accuracy of classifier on all the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "metrics_madras(model, expert.predict, n_dataset, val_loader)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The below are useful functions to implement a gumbel_softmax for approximating the probabilities, not used in above code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "gZugQfLgAJ7G"
   },
   "outputs": [],
   "source": [
    "from __future__ import print_function\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "import math\n",
    "\n",
    "def sample_gumbel(shape, eps=1e-20):\n",
    "    U = torch.rand(shape)\n",
    "    U = U.to(device)\n",
    "    return -Variable(torch.log(-torch.log(U + eps) + eps))\n",
    "\n",
    "def gumbel_softmax_sample(logits, temperature):\n",
    "    y = logits + sample_gumbel(logits.size())\n",
    "    return F.softmax(y / temperature, dim=-1)\n",
    "\n",
    "def gumbel_softmax(logits, temperature, hard=False):\n",
    "    \"\"\"\n",
    "    input: [*, n_class]\n",
    "    return: [*, n_class] an one-hot vector\n",
    "    \"\"\"\n",
    "    y = gumbel_softmax_sample(logits, temperature)\n",
    "    if hard:\n",
    "        shape = y.size()\n",
    "        _, ind = y.max(dim=-1)\n",
    "        y_hard = torch.zeros_like(y).view(-1, shape[-1])\n",
    "        y_hard.scatter_(1, ind.view(-1, 1), 1)\n",
    "        y_hard = y_hard.view(*shape)\n",
    "        return y_hard.view(-1, latent_dim * categorical_dim)\n",
    "    return y\n",
    "\n",
    "\n",
    "\n",
    "#print(gumbel_softmax(Variable(torch.tensor([[math.log(0.2), math.log(0.8)]] * 20000)),     0.0001).sum(dim=0))\n",
    "#gumbel_softmax(Variable(torch.tensor([math.log(0.5),math.log(0.5)])),0.8)\n",
    "\n",
    "\n",
    "def gumbel_binary_sample(logits, t=0.5, eps=1e-20):\n",
    "    \"\"\" Draw a sample from the Gumbel-Softmax distribution\"\"\"\n",
    "    gumbel_noise_on = sample_gumbel(logits.size())\n",
    "    gumbel_noise_off = sample_gumbel(logits.size())\n",
    "    concrete_on = (torch.log2(logits + eps) + gumbel_noise_on) / t\n",
    "    concrete_off = (torch.log2(1 - logits + eps) + gumbel_noise_off) / t\n",
    "    concrete_softmax = torch.div(torch.exp(concrete_on), torch.exp(concrete_on) + torch.exp(concrete_off))\n",
    "    return concrete_softmax\n"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "CIFAR10- madras baseline.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
